{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587aaa9f-d8b5-43cb-bfb6-79cd9e50a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfab76a-7b3e-4fe3-814e-bea281a82496",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Anomaly detection is a technique used in data analysis to identify unusual patterns, \n",
    "    outliers, or anomalous data points that deviate significantly from the expected or normal \n",
    "    behavior within a dataset. The purpose of anomaly detection is to identify these deviations, \n",
    "    which may represent critical insights, potential problems, or opportunities for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c4182-fb4f-4026-85a2-f188ba4167cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea224935-7ebb-4224-9aa4-c0e080aaec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Defining \"normal\" behavior: Determining what constitutes normal or expected behavior in a dataset \n",
    "    can be challenging, especially in complex systems or environments where patterns can be dynamic and evolving.\n",
    "    \n",
    "    Dealing with noise and outliers: Real-world data often contains noise, errors, or outliers that can interfere \n",
    "    with anomaly detection algorithms. Distinguishing between true anomalies and noise can be difficult.\n",
    "\n",
    "    Handling high-dimensional data: Many datasets contain a large number of features or variables, making \n",
    "    it challenging to identify anomalies in high-dimensional spaces.\n",
    "    \n",
    "    Adapting to concept drift: The definition of \"normal\" behavior may change over time, requiring anomaly \n",
    "    detection models to adapt and adjust to these changes (concept drift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd98c6-e8b3-44e0-8440-90a7540cd5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462739e-2e13-4405-8cea-d630190b1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. Unsupervised Anomaly Detection:\n",
    "    In unsupervised anomaly detection, the algorithm learns solely from the input data without any \n",
    "    labeled examples of anomalies or normal instances. The key characteristics of unsupervised anomaly \n",
    "    detection are:\n",
    "\n",
    "    No labeled data: The algorithm does not require pre-labeled data to train on. It learns the patterns \n",
    "    and structures present in the data itself.\n",
    "    Assumption of normality: Unsupervised methods assume that the majority of the data represents \"normal\" \n",
    "    instances, and anomalies are deviations from this normality.\n",
    "    Techniques: Common unsupervised anomaly detection techniques include clustering-based methods \n",
    "    (e.g., k-means), density-based methods (e.g., Local Outlier Factor), and statistical methods \n",
    "    (e.g., Gaussian mixture models).\n",
    "    Applications: Unsupervised methods are useful when labeled data is scarce or unavailable, or when the \n",
    "    definition of an anomaly is not well-defined or changes over time.\n",
    "    \n",
    "    Supervised Anomaly Detection:\n",
    "    In supervised anomaly detection, the algorithm is trained on a labeled dataset that contains examples \n",
    "    of both normal and anomalous instances. The key characteristics of supervised anomaly detection are:\n",
    "\n",
    "    Labeled data: The algorithm requires a labeled dataset where instances are explicitly marked as normal \n",
    "    or anomalous.\n",
    "    Classification approach: Supervised anomaly detection is typically framed as a binary classification \n",
    "    problem, where the algorithm learns to distinguish between normal and anomalous instances based on the \n",
    "    labeled examples.\n",
    "    Techniques: Common supervised techniques include decision trees, support vector machines, \n",
    "    neural networks, and other classification algorithms.\n",
    "    Applications: Supervised methods are suitable when labeled data is available or can be obtained through \n",
    "    manual labeling or domain expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5d2f5-9e08-4591-ac4d-9861eb8394d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae3935-3a54-4f9c-a931-96c59777a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Statistical methods:\n",
    "    These methods model the normal behavior of the data using statistical techniques and identify \n",
    "    instances that deviate significantly from this model as anomalies.\n",
    "    Examples: Gaussian mixture models, parametric and non-parametric techniques, hypothesis testing.\n",
    "    \n",
    "    Proximity-based methods:\n",
    "    These methods identify anomalies based on their distance or similarity to the majority of the data \n",
    "    points.\n",
    "    Examples: k-Nearest Neighbors (k-NN), distance-based outlier detection.\n",
    "    \n",
    "    Density-based methods:\n",
    "    These methods assume that normal instances occur in dense regions, while anomalies lie in sparse \n",
    "    regions of the data.\n",
    "    Examples: Local Outlier Factor (LOF), Cluster-Based Local Outlier Factor (CBLOF), Density-Based \n",
    "    Spatial Clustering of Applications with Noise (DBSCAN).\n",
    "    \n",
    "    Subspace and Projection methods:\n",
    "    These methods identify anomalies by projecting the data into different subspaces or lower-dimensional spaces, where anomalies may be more easily detectable.\n",
    "    Examples: Principal Component Analysis (PCA), Subspace Outlier Detection.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a74423-df4f-4d5b-b06e-8ea291236fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85527ff1-5a29-48bc-8de9-6b7410ed680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Proximity assumption: The fundamental assumption is that normal data instances are close together, \n",
    "    forming dense clusters or regions, while anomalies are far away from these dense regions. This means that \n",
    "    anomalies are expected to have larger distances from their nearest neighbors compared to normal instances.\n",
    "\n",
    "    Representative training data: It is assumed that the training data is representative of the normal \n",
    "    instances and does not contain a significant number of anomalies. If the training data is contaminated \n",
    "    with too many anomalies, the distance-based methods may not accurately model the normal behavior.\n",
    "\n",
    "    Meaningful distance metric: These methods rely on the existence of a meaningful distance or similarity \n",
    "    metric that can accurately measure the proximity between data instances. The choice of distance metric \n",
    "    (e.g., Euclidean distance, Manhattan distance, cosine similarity) should be appropriate for the specific \n",
    "    data and problem domain.\n",
    "    \n",
    "    Equal variance assumption (for some methods): Some distance-based methods, such as the classic k-NN, \n",
    "    assume that the features have equal variance or importance in determining the distance between \n",
    "    instances. This assumption may not hold true for all datasets, and violations can lead to suboptimal \n",
    "    performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74d240-ef08-44e8-8d85-b3d3413e493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "A6. Compute k-distance: For each data instance, the k-distance is calculated, which is the distance \n",
    "    to the k-th nearest neighbor. This gives an idea of the density around that instance based on its \n",
    "    nearest neighbor distances.\n",
    "    \n",
    "    Compute reachability distance: The reachability distance of instance A with respect to instance B \n",
    "    is the maximum of the k-distance of B and the actual distance between A and B. This is used to reduce \n",
    "    the impact of statistical fluctuations in dense regions.\n",
    "    \n",
    "    Compute local reachability density: The local reachability density of an instance is the inverse of \n",
    "    the average reachability distance from its k nearest neighbors. It captures the idea that instances \n",
    "    with a higher density around them have a higher local reachability density.\n",
    "    \n",
    "    Compute LOF score: The LOF score of an instance is calculated by comparing its local reachability \n",
    "    density to the local reachability densities of its k nearest neighbors: \n",
    "        LOF(A) = sum(local_reachability_density(B) / local_reachability_density(A)) / k \n",
    "        Where the sum is taken over the k nearest neighbors of instance A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14604a-6670-4f9b-b601-c57dcd4c7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "A7. Number of trees (n_estimators): This parameter specifies the number of isolation trees to build in \n",
    "    the ensemble. A higher number of trees generally leads to better anomaly detection performance, but at \n",
    "    the cost of increased computational complexity and memory usage. Typical values range from 100 to 500 \n",
    "    trees.\n",
    "    \n",
    "    Subsample size (max_samples): This parameter determines the size of the subsample of instances used \n",
    "    to construct each isolation tree. By default, it is set to the smaller of 256 or the number of \n",
    "    instances in the dataset. A smaller subsample size can lead to better anomaly detection performance, \n",
    "    but may also increase the risk of overfitting to the training data.\n",
    "\n",
    "    Maximum tree depth (max_depth): This parameter limits the maximum depth of the isolation trees. \n",
    "    Setting a lower value can help control the complexity of the trees and prevent overfitting, \n",
    "    but may also reduce the ability to capture subtle anomalies.\n",
    "    \n",
    "    Contamination (contamination): This parameter specifies the expected proportion of anomalies in \n",
    "    the dataset. It is used to define the threshold for flagging instances as anomalies based on their \n",
    "    anomaly scores. A higher value will result in more instances being labeled as anomalies.\n",
    "    \n",
    "    Bootstrap (bootstrap): This parameter determines whether bootstrap samples are used when constructing \n",
    "    each isolation tree. Setting it to True can improve the diversity of the trees and potentially \n",
    "    enhance anomaly detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376c77a-736b-4c12-9831-bc92cb1c3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "    using KNN with K=10?\n",
    "A8. Since we are using k-NN with k=10, we need to consider the 10 nearest neighbors of the data point.\n",
    "    Within a radius of 0.5, the data point has only 2 neighbors of the same class.\n",
    "    This means that the remaining 8 neighbors (out of the 10 nearest neighbors) are of different classes \n",
    "    or are at a distance greater than 0.5.\n",
    "    In k-NN anomaly detection, a data point is considered an anomaly if the majority of its k nearest \n",
    "    neighbors are different from its own class or are far away.\n",
    "    In this case, 8 out of the 10 nearest neighbors are either of a different class or are far away \n",
    "    (beyond the radius of 0.5).\n",
    "    Therefore, the anomaly score for this data point would be relatively high, as it deviates \n",
    "    significantly from its neighbors in terms of class or distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935c0bd-ca62-48ae-a735-917e6b0067b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "    anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "    length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57b059-fb1a-436a-971f-efa8968c6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9. The anomaly score = 2^(avg_path_length)/c(m)\n",
    "    c(m) = 2⋅(ln(n−1)+0.5772)\n",
    "    \n",
    "    avg_path_length=5\n",
    "    c(3000)=17.16\n",
    "    \n",
    "    anomaly score=2^(-5)/17.16\n",
    "    \n",
    "    The anomaly score is Anomaly Score ≈ 0.747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fcf88-a6b7-4c0f-a733-19b56bb16330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
